#%matplotlib inline 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
from pattern import web
from datetime import date
from datetime import datetime
import re
from sklearn.preprocessing import MinMaxScaler
from sklearn.cross_validation import LeaveOneOut
from sklearn.metrics import (mean_squared_error, mean_absolute_error)
from sklearn.ensemble import ExtraTreesClassifier 
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Ridge 
from sklearn.linear_model import LinearRegression
import statsmodels as sm
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.grid_search import (GridSearchCV, RandomizedSearchCV)

test= pd.io.parsers.read_csv("/Users/Palinzhou/Desktop/Python/project/test.csv")
train=pd.io.parsers.read_csv("/Users/Palinzhou/Desktop/Python/project/train.csv")
#print train.shape
#print test.shape
Fake_Ratio = 311.5
column1=['Id','Opendate','City','Citygroup','Type','P1','P2','P3', 'P4', 'P5',
         'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12',
         'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21',
         'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30',
         'P31', 'P32', 'P33', 'P34','P35', 'P36', 'P37', 'Revenue']
column2=['Id','Opendate','City','Citygroup','Type','P1','P2','P3', 'P4', 'P5',
         'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12',
         'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21',
         'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P29', 'P30',
         'P31', 'P32', 'P33', 'P34','P35', 'P36', 'P37']
train.columns= column1
test.columns= column2
#concate train set and test set
total_data= pd.concat((test, train), ignore_index=True)
print total_data.columns
print total_data
#basic information
print train.shape
print test.shape
print train.describe()
print test.describe()

#transform Open date to days value till now as an integer 
arr= []
for d in total_data["Opendate"]:
    l = datetime.now() - datetime.strptime(d, '%m/%d/%Y')
    pattern = re.compile(r'(\d*\s)')
    le = pattern.search(str(l))
    arr.append(int(le.group())) 
total_data["OpenToNow"]= arr
'''arr2= []
for d in test["Opendate"]:
    l = datetime.now() - datetime.strptime(d, '%m/%d/%Y')
    pattern = re.compile(r'(\d*\s)')
    le = pattern.search(str(l))
    arr2.append(int(le.group())) 
test["OpenToNow"]= arr2'''
#print train["OpenToNow"]
#print(train.describe())
#Log transformation of days value till now for open date
total_data["LogDaysToOpen"]=total_data["OpenToNow"].apply(np.log)
#test["LogDaysToOpen"]=test["OpenToNow"].apply(np.log)
#total_data= pd.concat((test, train), ignore_index=True)
total_data = total_data.drop(["OpenToNow","Opendate","Id"], axis=1)
print total_data.columns

#plot the distribution of opendate and revenue
train = total_data.ix[pd.notnull(total_data.Revenue)]
test = total_data.ix[pd.isnull(total_data.Revenue)]
plt.rcParams['figure.figsize'] = (8, 6)
train[["LogDaysToOpen", "Revenue"]].plot(x="LogDaysToOpen", y="Revenue", kind='scatter', title="Log of Days Opened vs Revenue")

# Deal with City, replace the city value with frequency count occured in train and test set
counts = (test["City"].value_counts() / Fake_Ratio).add(train["City"].value_counts(), fill_value=0)
total_data["City"] = total_data["City"].replace(counts)
#print total_data["City"].head()
total_data["LogOfResCountInCity"] = np.log(total_data["City"])
#drop the original city column
total_data = total_data.drop(["City"], axis=1)
# plot the restaurant count in city with scatter distribution 
plt.rcParams['figure.figsize'] = (8, 6) # Resize plots
total_data[["LogOfResCountInCity", "Revenue"]].plot(x="LogOfResCountInCity", y="Revenue", kind='scatter', title="Log of Restaurant Count In City vs Revenue",color="red")

#Deal with the missing value
revenue= total_data.Revenue
total_data = total_data.drop(["Revenue"], axis=1)
missing_columns = ['P14', 'P15', 'P16', 'P17', 'P18', 'P24', 'P25', 'P26', 'P27', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37']
total_data['Missing'] = (total_data[missing_columns] == 0).sum(axis=1)
total_data = total_data.join(revenue)
#plot missing value distribution in train set and test set
plt.rcParams['figure.figsize'] = (20, 8)# Resize plots
fig, axs = plt.subplots(1,2)
print "Distribution of missing values in train set and test set:"
total_data['Missing'].ix[pd.notnull(total_data.Revenue)].value_counts().plot(title="Train Set", kind='bar', ax=axs[0],color="green")
total_data['Missing'].ix[pd.isnull(total_data.Revenue)].value_counts().plot(title="Test Set", kind='bar', ax=axs[1], color='red')
plt.show()
# for missing values, transform to mean value of that column
for miss_col in missing_columns:
    total_data.loc[total_data[miss_col] == 0, [miss_col]] = np.mean(total_data[miss_col])

#Here convert two categorical variables "Citygroup" and "Type" to dummy variables
plt.rcParams['figure.figsize'] = (16, 8)# Resize plots
fig, axs = plt.subplots(1,2)
total_data['Citygroup'].ix[pd.notnull(total_data.Revenue)].value_counts().plot(title="City Group Distribution in the Train Set", kind='bar', ax=axs[0],color="green") 
total_data['Citygroup'].ix[pd.isnull(total_data.Revenue)].value_counts().plot(title="City Group Distribution in the Test Set", kind='bar', ax=axs[1],color="red")
plt.show()

plt.rcParams['figure.figsize'] = (16, 8)
fig, axs = plt.subplots(1,2)
total_data['Type'].ix[pd.notnull(total_data.Revenue)].value_counts().plot(title="Restaurant Type Distribution in the Train Set", kind='bar', ax=axs[0],color="blue")
(total_data['Type'].ix[pd.isnull(total_data.Revenue)].value_counts()/Fake_Ratio).plot(title="Approximate Restaurant Type Distribution in the Test Set", kind='bar', ax=axs[1],color="black")   
plt.show()
#According to distribution, type "MB" not in train set, but in test set. so convert the type MB to DT in test set
total_data.loc[total_data.Type == 'MB', ['Type']] = 'DT'

#Then get the dummy variables about "Citygroup" and "Type"
total_data = total_data.join(pd.get_dummies(total_data['Citygroup'], prefix="CG"))
total_data = total_data.join(pd.get_dummies(total_data['Type'], prefix="T"))
#drop the original column and one less value column for dummy variables about "Citygroup" and "Type"
total_data = total_data.drop(["Citygroup", "Type", "CG_Other", "T_DT"], axis=1)

# Fit_data are the rows in the train set that belong to one of the common restaurnat types "FC" and "IL"
Fit_data = total_data.ix[((total_data.T_FC==1) | (total_data.T_IL==1)) & (pd.notnull(total_data.Revenue))]
# Typetofill are rows with types belong to rare type "DT"
Typetofill = total_data.ix[((total_data.T_FC==0) & (total_data.T_IL==0))]
# Retaurants with type FC are labeled 1, those with type IL are labeled 0.
y = Fit_data.T_FC
x = Fit_data.drop(["T_FC", "T_IL", "Revenue"], axis=1)

Gridmodel = {'max_depth': [None, 8], 'max_features':['sqrt', 0.5, None],'min_samples_split': [4,9,16], 'min_samples_leaf':[1,4]}
Type_Model = ExtraTreesClassifier(n_estimators=25, random_state= 0)
grid = RandomizedSearchCV(Type_Model, Gridmodel, n_iter=10, cv=5, scoring="roc_auc")
grid.fit(x,y)

Type_Model.set_params(**grid.best_params_)
Type_Model.fit(x, y)

predt = Type_Model.predict(Typetofill.drop(["T_FC", "T_IL", "Revenue"], axis=1))
total_data.loc[(total_data.T_FC==0) & (total_data.T_IL==0), "T_FC"] = predt
total_data = total_data.drop(["T_IL"], axis=1)

'''
#Square root transformation applied to  obfuscated P variables with maximum value >= 10, to make them into the same scale, as well as the target variable Revenue
for column in total_data.columns:
    if column[0] == 'P':
        total_data.loc[total_data[column] >= 10, [column]] = np.round(np.sqrt(total_data[column]),1)
'''        
# convert P variables to dummy variables
for pcol in total_data.columns:
    if pcol[0] == 'P':
        dummy_pcol= pd.get_dummies(total_data[pcol], prefix=pcol)
        total_data = total_data.join(dummy_pcol)
        total_data = total_data.drop([pcol, total_data.columns[-1]], axis=1)

#Scale input features to values between 0 and 1
scaler = MinMaxScaler()
#convert Revenue value to make sqrt to reduce the influence from large range with a few large revenue values
revenue = np.sqrt(total_data.Revenue)
#Drop Revenue column and preprocess all data except Revenue
total_data = total_data.drop(['Revenue'], axis=1)
total_data = pd.DataFrame(data =scaler.fit_transform(total_data), columns = total_data.columns, index=total_data.index)
#After fit_transform, include revenue into final dataset
total_data = total_data.join(revenue)
# Get the train set and test set, prepare the independent variables and dependent variable for regression model
train = total_data.ix[pd.notnull(total_data.Revenue)]
train_y=train.Revenue
train_x= train.drop(['Revenue'], axis=1)
test = total_data.ix[pd.isnull(total_data.Revenue)].drop(['Revenue'], axis=1)

#Linear Regression Model to find important factor
regression = LinearRegression(normalize=True)
regression.fit(train_x,train_y)
print regression.score(train_x,train_y)
dic ={}
for var, cof in zip(train_x.columns, regression.coef_,):
    dic[var]=str(round(cof,1))
#print dic
#remove the unimportant factor
print len(total_data.columns)
for var in dic:
    if dic[var]=='0.0':
        total_data = total_data.drop([var], axis=1) 
print len(total_data.columns)  
# make the train set and test set ready with important factor
train = total_data.ix[pd.notnull(total_data.Revenue)]
train_y=train.Revenue
train_x= train.drop(['Revenue'], axis=1)
test = total_data.ix[pd.isnull(total_data.Revenue)].drop(['Revenue'], axis=1)
regression.fit(train_x,train_y)
#Predict with linear Regression model
submission0 = pd.DataFrame(columns=['Prediction'],index=test.index, data=regression.predict(test))
# Convert back to revenue from sqrt(revenue) before
submission0.Prediction = np.square(submission0.Prediction)
# set column name
submission0.index.name='Id'
# Write out the submission into csv file
submission0.to_csv("/Users/Palinzhou/Desktop/Python/project/TFI_Regression.csv")
# Sanity check on the submission
submission0.describe().astype(int)

# Predict with a Ridge Regression model
Gridmodel = [{'normalize': [True, False], 'alpha': np.logspace(0,10)}]
Ridge_model = Ridge()
grid = GridSearchCV(Ridge_model, Gridmodel, cv=LeaveOneOut(len(train_y)), scoring='mean_squared_error')
grid.fit(train_x, train_y)
Ridge_model.set_params(**grid.best_params_)
Ridge_model.fit(train_x, train_y)

# Predict on the test set with the trained model.
submission1 = pd.DataFrame(columns=['Prediction'],index=test.index, data=Ridge_model.predict(test))
submission1.Prediction = np.square(submission1.Prediction)
submission1.index.name='Id'
submission1.to_csv("/Users/Palinzhou/Desktop/Python/project/TFI_Ridge2.csv")
submission1.describe().astype(int)

#Use the decision tree model to compare the prediction
DT=tree.DecisionTreeRegressor()
DT=DT.fit(train_x,train_y)
submission2 = pd.DataFrame(columns=['Prediction'],index=test.index, data=DT.predict(test))
submission2.Prediction = np.square(submission2.Prediction)
submission2.index.name='Id'
# Write out the submission
submission2.to_csv("/Users/Palinzhou/Desktop/Python/project/TFI_DecisionTree.csv")
submission2.describe().astype(int)

'''
#Predict with Random Forest Model
RF=RandomForestClassifier(n_estimators=100)
RF.fit(train_x, train_y)
submission3 = pd.DataFrame(columns=['Prediction'],index=test.index, data=RF.predict(test))
submission3.Prediction = np.square(submission3.Prediction)
submission3.index.name='Id'
submission3.to_csv("/Users/Palinzhou/Desktop/Python/project/TFI_RandomForrest.csv")
submission2.describe().astype(int)
'''

#logistical regression model
'''
logit =sm.logit(train_y,train_x)
#fit the model
result=logit.fit()
print result.summary()
'''
